{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "322f31f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "# from patchify import patchify\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import tifffile\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from keras.models import Model\n",
    "# from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf1895e",
   "metadata": {
    "id": "faf1895e"
   },
   "source": [
    "# Reading Stacked Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fdf2f300",
   "metadata": {
    "executionInfo": {
     "elapsed": 32856,
     "status": "ok",
     "timestamp": 1672678143999,
     "user": {
      "displayName": "Touseef Ur Rehman",
      "userId": "07158742900075865006"
     },
     "user_tz": -300
    },
    "id": "fdf2f300"
   },
   "outputs": [],
   "source": [
    "DIRECTORY = (r\"C:\\Users\\tause\\Advance_ANN\\Stacked\")\n",
    "\n",
    "CATEGORIES = []\n",
    "for classes in os.listdir(DIRECTORY):\n",
    "    CATEGORIES += [classes]\n",
    "\n",
    "# print(CATEGORIES)\n",
    "\n",
    "data = []\n",
    "i=0\n",
    "for category in CATEGORIES:\n",
    "    img_path = os.path.join(DIRECTORY, category)\n",
    "    arr = cv2.imread(img_path,0)\n",
    "    data.append(arr)\n",
    "\n",
    "stacked = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "34d852d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1672678160795,
     "user": {
      "displayName": "Touseef Ur Rehman",
      "userId": "07158742900075865006"
     },
     "user_tz": -300
    },
    "id": "34d852d4",
    "outputId": "da9718d1-2103-4420-e111-cd4a544aa191"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 512, 32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked = np.transpose(stacked, (1, 2, 0))\n",
    "stacked.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4138ce83",
   "metadata": {
    "id": "4138ce83"
   },
   "source": [
    "# Making Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b0e412ab",
   "metadata": {
    "executionInfo": {
     "elapsed": 1720,
     "status": "ok",
     "timestamp": 1672678172049,
     "user": {
      "displayName": "Touseef Ur Rehman",
      "userId": "07158742900075865006"
     },
     "user_tz": -300
    },
    "id": "b0e412ab",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_blocks(image, block_size, stride):\n",
    "    blocks = []\n",
    "    image_height, image_width = image.shape[:2]\n",
    "    num_blocks_x = image_width // stride\n",
    "    num_blocks_y = image_height // stride\n",
    "    for y in range(0,image_height-block_size[1]+1,stride):\n",
    "        for x in range(0,image_width-block_size[0]+1,stride):\n",
    "            block=image[y:y+block_size[1],x:x+block_size[0]]\n",
    "            blocks.append(block)\n",
    "    return blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "q8yk7ZQsFV0C",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1672678172876,
     "user": {
      "displayName": "Touseef Ur Rehman",
      "userId": "07158742900075865006"
     },
     "user_tz": -300
    },
    "id": "q8yk7ZQsFV0C"
   },
   "outputs": [],
   "source": [
    "def reconstruct_image(patches, image_size, block_size, stride):\n",
    "    # Get patch size\n",
    "    patch_size = block_size[0]\n",
    "\n",
    "    # Calculate the number of patches in each direction\n",
    "    n_patches_x = (image_size[1] - patch_size) // stride + 1\n",
    "    n_patches_y = (image_size[0] - patch_size) // stride + 1\n",
    "\n",
    "    # Initialize reconstructed image\n",
    "    reconstructed_image = np.zeros(image_size)\n",
    "\n",
    "    # Place patches in the reconstructed image\n",
    "    patch_idx = 0\n",
    "    for y in range(n_patches_y):\n",
    "        for x in range(n_patches_x):\n",
    "            patch = patches[patch_idx]\n",
    "            reconstructed_image[y*stride:y*stride+patch_size, x*stride:x*stride+patch_size] = patch\n",
    "            patch_idx += 1\n",
    "\n",
    "    return reconstructed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6c9ebda8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1672678175208,
     "user": {
      "displayName": "Touseef Ur Rehman",
      "userId": "07158742900075865006"
     },
     "user_tz": -300
    },
    "id": "6c9ebda8",
    "outputId": "14dee7cf-08e5-4903-b216-2d049bc82c17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "361"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches = np.array(extract_blocks(stacked,(64,64),24))\n",
    "len(patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa69dfed",
   "metadata": {
    "id": "aa69dfed"
   },
   "source": [
    "# Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2aea7a0d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1672678184405,
     "user": {
      "displayName": "Touseef Ur Rehman",
      "userId": "07158742900075865006"
     },
     "user_tz": -300
    },
    "id": "2aea7a0d",
    "outputId": "023b6422-30fa-4676-ca48-f579279a5050"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(361, 64, 64, 32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(patches)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7c58f116",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1672678185360,
     "user": {
      "displayName": "Touseef Ur Rehman",
      "userId": "07158742900075865006"
     },
     "user_tz": -300
    },
    "id": "7c58f116"
   },
   "outputs": [],
   "source": [
    "y = y.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fa51f8",
   "metadata": {
    "id": "b6fa51f8"
   },
   "source": [
    "# Making Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2c81f531",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 768,
     "status": "ok",
     "timestamp": 1672678186948,
     "user": {
      "displayName": "Touseef Ur Rehman",
      "userId": "07158742900075865006"
     },
     "user_tz": -300
    },
    "id": "2c81f531",
    "outputId": "50b00649-affb-4121-fac0-a2f237ab54b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([361, 8, 8, 32])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = tf.keras.layers.AveragePooling2D(pool_size=(8, 8),strides=(8, 8), padding='valid',dtype='float64')\n",
    "x1 = x1(y)\n",
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "00bb351c",
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1672678186949,
     "user": {
      "displayName": "Touseef Ur Rehman",
      "userId": "07158742900075865006"
     },
     "user_tz": -300
    },
    "id": "00bb351c"
   },
   "outputs": [],
   "source": [
    "x2_1 = y[:,:,:,0:11].mean(axis = 3)\n",
    "x2_2 = y[:,:,:,11:22].mean(axis = 3)\n",
    "x2_3 = y[:,:,:,22:].mean(axis = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "24e25d9c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1672678186951,
     "user": {
      "displayName": "Touseef Ur Rehman",
      "userId": "07158742900075865006"
     },
     "user_tz": -300
    },
    "id": "24e25d9c",
    "outputId": "6233eca7-2378-4e62-b1a6-177ec0f77ded"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(361, 64, 64, 3)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = np.concatenate([np.expand_dims(x2_1,3),np.expand_dims(x2_2,3),np.expand_dims(x2_3,3)],axis = 3)\n",
    "x2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5dbb81",
   "metadata": {
    "id": "9f5dbb81"
   },
   "source": [
    "# PreProcessing for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3f1d5e0c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1672678186952,
     "user": {
      "displayName": "Touseef Ur Rehman",
      "userId": "07158742900075865006"
     },
     "user_tz": -300
    },
    "id": "3f1d5e0c",
    "outputId": "8c2cf924-2640-4c15-dee7-8ab7a2081d3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels-shape (361, 64, 64, 32)\n",
      "Input_Feature-1 shape (361, 8, 8, 32)\n",
      "Input_Feature-2 shape (361, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Labels-shape\",y.shape)\n",
    "print(\"Input_Feature-1 shape\",x1.shape)\n",
    "print(\"Input_Feature-2 shape\",x2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c5d4f1",
   "metadata": {
    "id": "42c5d4f1"
   },
   "source": [
    "# Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c06c2ee9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1476,
     "status": "ok",
     "timestamp": 1672678191634,
     "user": {
      "displayName": "Touseef Ur Rehman",
      "userId": "07158742900075865006"
     },
     "user_tz": -300
    },
    "id": "c06c2ee9",
    "outputId": "b1c55117-21d8-42a1-b685-e05a93b6afa4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([361, 64, 64, 32])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_interploated = tf.image.resize(x1, (64, 64), method='bilinear')\n",
    "X_interploated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cc586ecf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3259,
     "status": "ok",
     "timestamp": 1672678194890,
     "user": {
      "displayName": "Touseef Ur Rehman",
      "userId": "07158742900075865006"
     },
     "user_tz": -300
    },
    "id": "cc586ecf",
    "outputId": "ec8b78f9-36d0-458c-8371-e6dc2f172ac0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([361, 64, 64, 35])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.concat([X_interploated, x2], axis=-1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "916573f6",
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1672678194891,
     "user": {
      "displayName": "Touseef Ur Rehman",
      "userId": "07158742900075865006"
     },
     "user_tz": -300
    },
    "id": "916573f6"
   },
   "outputs": [],
   "source": [
    "X = X/255\n",
    "y = y/255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d9b684",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478bebad",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f4ef42e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = keras.Sequential(\n",
    "    [\n",
    "        layers.Input((64, 64, 35)),\n",
    "\n",
    "        layers.Conv2D(filters = 32,kernel_size = (3,3),strides=1,padding=\"valid\",activation=\"LeakyReLU\"),\n",
    "        #shape(batch_size, 62, 62, 32).\n",
    "        \n",
    "        layers.Conv2D(filters = 64,kernel_size = (3,3),strides=1,padding=\"valid\",activation=\"LeakyReLU\"),\n",
    "        #(batch_size, 60, 60, 64).\n",
    "\n",
    "        layers.Conv2D(filters = 64,kernel_size = (3,3),strides=1,padding=\"valid\",activation=\"LeakyReLU\"),\n",
    "        #(batch_size, 58, 58, 64).\n",
    "\n",
    "        layers.Conv2D(filters = 128,kernel_size = (3,3),strides=1,padding=\"valid\",activation=\"LeakyReLU\"),\n",
    "        #(batch_size, 56, 56, 128).\n",
    "        \n",
    "        layers.Reshape((112 ,112 ,32)),\n",
    "        \n",
    "        layers.Conv2D(filters = 32,kernel_size = (49,49),strides=1,padding=\"valid\",activation=\"LeakyReLU\"),\n",
    "        #(batch_size, 64, 64, 32).\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n",
    "\n",
    "# Create the generator.\n",
    "generator = keras.Sequential(\n",
    "    [\n",
    "        layers.Input((64, 64, 35)),\n",
    "\n",
    "        layers.Conv2D(filters = 32,kernel_size = (3,3),strides=1,padding=\"valid\",activation=\"LeakyReLU\"),\n",
    "        #shape(batch_size, 62, 62, 32).\n",
    "        \n",
    "        layers.Conv2D(filters = 64,kernel_size = (3,3),strides=1,padding=\"valid\",activation=\"LeakyReLU\"),\n",
    "        #(batch_size, 60, 60, 64).\n",
    "\n",
    "        layers.Conv2D(filters = 64,kernel_size = (3,3),strides=1,padding=\"valid\",activation=\"LeakyReLU\"),\n",
    "        #(batch_size, 58, 58, 64).\n",
    "\n",
    "        layers.Conv2D(filters = 128,kernel_size = (3,3),strides=1,padding=\"valid\",activation=\"LeakyReLU\"),\n",
    "        #(batch_size, 56, 56, 128).\n",
    "        \n",
    "        layers.Reshape((112 ,112 ,32)),\n",
    "        \n",
    "        layers.Conv2D(filters = 32,kernel_size = (49,49),strides=1,padding=\"valid\",activation=\"LeakyReLU\"),\n",
    "        #(batch_size, 64, 64, 32).\n",
    "    ],\n",
    "    name=\"generator\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7d595e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the discriminator loss function\n",
    "def discriminator_loss(real_output, synthetic_output):\n",
    "    real_loss = tf.kerasbinary_crossentropy(tf.ones_like(real_output), real_output)\n",
    "    synthetic_loss = tf.kerasbinary_crossentropy(tf.zeros_like(synthetic_output), synthetic_output)\n",
    "    total_loss = real_loss + synthetic_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dcf26d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generator loss function\n",
    "def generator_loss(synthetic_output):\n",
    "    return tf.kerasbinary_crossentropy(tf.ones_like(synthetic_output), synthetic_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6a140161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the discriminator and generator optimizers\n",
    "discriminator_optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "generator_optimizer = keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6a991164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training loop\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([64,64,64,35])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        synthetic_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        synthetic_output = discriminator(synthetic_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(synthetic_output)\n",
    "        disc_loss = discriminator_loss(real_output, synthetic_output)\n",
    "\n",
    "        gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "        gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "        generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "        discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c8e6f1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 35)\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "# BATCH_SIZE = 64\n",
    "# noise_dim = [64,64,32]\n",
    "# Train the GAN\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for images in X:\n",
    "        print(images.shape)\n",
    "        break\n",
    "    break\n",
    "#         np.expand_dims(images,axis=0)\n",
    "#         train_step((np.expand_dims(images,axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "39329d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "class Generator(keras.Model):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = keras.layers.Conv2D(64, 3, padding='same')\n",
    "        self.conv2 = keras.layers.Conv2D(128, 3, padding='same')\n",
    "        self.conv3 = keras.layers.Conv2D(256, 3, padding='same')\n",
    "        self.conv4 = keras.layers.Conv2D(512, 3, padding='same')\n",
    "        self.conv5 = keras.layers.Conv2D(out_channels, 3, padding='same')\n",
    "        self.bn1 = keras.layers.BatchNormalization()\n",
    "        self.bn2 = keras.layers.BatchNormalization()\n",
    "        self.bn3 = keras.layers.BatchNormalization()\n",
    "        self.bn4 = keras.layers.BatchNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.nn.relu(self.bn1(self.conv1(x)))\n",
    "        x = tf.nn.relu(self.bn2(self.conv2(x)))\n",
    "        x = tf.nn.relu(self.bn3(self.conv3(x)))\n",
    "        x = tf.nn.relu(self.bn4(self.conv4(x)))\n",
    "        x = tf.nn.tanh(self.conv5(x))\n",
    "        return x\n",
    "\n",
    "class Discriminator(keras.Model):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = keras.layers.Conv2D(64, 3, padding='same')\n",
    "        self.conv2 = keras.layers.Conv2D(128, 3, padding='same')\n",
    "        self.conv3 = keras.layers.Conv2D(256, 3, padding='same')\n",
    "        self.conv4 = keras.layers.Conv2D(512, 3, padding='same')\n",
    "        self.conv5 = keras.layers.Conv2D(1, 3, padding='same')\n",
    "        self.bn1 = keras.layers.BatchNormalization()\n",
    "        self.bn2 = keras.layers.BatchNormalization()\n",
    "        self.bn3 = keras.layers.BatchNormalization()\n",
    "        self.bn4 = keras.layers.BatchNormalization()\n",
    "    def call(self, x):\n",
    "        x = tf.nn.leaky_relu(self.bn1(self.conv1(x)), alpha=0.2)\n",
    "        x = tf.nn.leaky_relu(self.bn2(self.conv2(x)), alpha=0.2)\n",
    "        x = tf.nn.leaky_relu(self.bn3(self.conv3(x)), alpha=0.2)\n",
    "        x = tf.nn.leaky_relu(self.bn4(self.conv4(x)), alpha=0.2)\n",
    "        x = tf.nn.sigmoid(self.conv5(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "98d6a8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the input shapes for the generator and discriminator\n",
    "generator_input_shape = (None, 64, 64, 35)\n",
    "discriminator_input_shape = (None, 64, 64, 32)\n",
    "\n",
    "# create instances of the generator and discriminator\n",
    "generator = Generator(35, 32)\n",
    "discriminator = Discriminator(32)\n",
    "\n",
    "# define the optimizers for the generator and discriminator\n",
    "generator_optimizer = keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_optimizer = keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "# define the loss functions for the generator and discriminator\n",
    "loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "# define the training loop\n",
    "@tf.function\n",
    "def train_step(inputs, labels):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(inputs, training=True)\n",
    "\n",
    "        real_output = discriminator(labels, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = loss_fn(tf.ones_like(fake_output), fake_output)\n",
    "        disc_loss = loss_fn(tf.ones_like(real_output), real_output) + loss_fn(tf.zeros_like(fake_output), fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "# define a function to generate and save images during training\n",
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    predictions = model(test_input, training=False)\n",
    "\n",
    "    def generate_and_save_images(model, epoch, test_input):\n",
    "        predictions = model(test_input, training=False)\n",
    "\n",
    "        fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "        for i in range(predictions.shape[0]):\n",
    "            plt.subplot(4, 4, i+1)\n",
    "            plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "            plt.axis('off')\n",
    "\n",
    "        plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c7e9b957",
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\users\\tause\\envs\\classical\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:766\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 766\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n",
      "File \u001b[1;32mc:\\users\\tause\\envs\\classical\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:749\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[1;32m--> 749\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    754\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    755\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\tause\\envs\\classical\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3016\u001b[0m, in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   3015\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 3016\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3017\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n",
      "File \u001b[1;32mc:\\users\\tause\\envs\\classical\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:7164\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7163\u001b[0m e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 7164\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[1;31mOutOfRangeError\u001b[0m: End of sequence [Op:IteratorGetNext]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[104], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# get a batch of data\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# train the model on the batch\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     train_step(inputs, labels)\n",
      "File \u001b[1;32mc:\\users\\tause\\envs\\classical\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:768\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    766\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_internal()\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[1;32m--> 768\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define the number of epochs and the batch size\n",
    "num_epochs = 100\n",
    "batch_size = 16\n",
    "test_input = tf.random.normal([16, 64, 64, 35])\n",
    "# create a dataset and a data iterator\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X, y)).batch(batch_size)\n",
    "data_iterator = iter(dataset)\n",
    "\n",
    "# train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # get a batch of data\n",
    "    inputs, labels = next(data_iterator)\n",
    "\n",
    "    # train the model on the batch\n",
    "    train_step(inputs, labels)\n",
    "\n",
    "    # generate and save images every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        generate_and_save_images(generator, epoch, test_input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6d0e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
